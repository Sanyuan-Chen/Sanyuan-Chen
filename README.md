### Hi, I'm Sanyuan Chen ðŸ‘‹

[![Homepage](https://img.shields.io/badge/Homepage-7D4698?style=flat-square&logo=asana&logoColor=white)](https://sanyuan-chen.github.io)
[![GitHub](https://img.shields.io/badge/GitHub-gray?style=flat-square&logo=github&logoColor=white)](https://github.com/Sanyuan-Chen)
[![Scholar Badge](https://img.shields.io/badge/Google-%230288D1?style=flat-square&logo=googlescholar&logoColor=white&link=https://scholar.google.com/citations?user=XrZRIy0AAAAJ)](https://scholar.google.com/citations?user=XrZRIy0AAAAJ)
[![Linkedin Badge](https://img.shields.io/badge/Linkedin-blue?style=flat-square&logo=Linkedin&logoColor=white&link=https://www.linkedin.com/in/sanyuan-chen-08a495167/)](https://www.linkedin.com/in/sanyuan-chen-08a495167/)
[![Zhihu](https://img.shields.io/badge/Zhihu-%230288D1?style=flat-square&logo=zhihu&logoColor=white)](https://www.zhihu.com/people/mypleasure)
[![Gmail](https://img.shields.io/badge/Email-8B89CC?style=flat-square&logo=microsoftoutlook&logoColor=white)](mailto:t-schen@microsoft.com)


- ðŸŽ“ Iâ€™m currently a Ph.D. student at Harbin Institute of Technology and a research intern in Microsoft Research Asia.
- ðŸŒ± My research interests include self-supervised learning, speech and audio processing and spoken language processing. 
- ðŸ“„ My research highlights: 

   - [Jan 2023] [**VALL-E**](https://arxiv.org/abs/2301.02111), a language modeling approach for text to speech synthesis, achieves **state-of-the-art** zero-shot TTS performance and emerges **in-context learning** capabilities. See https://aka.ms/valle for demos.
   
   - [Dec 2022] [**BEATs**](https://arxiv.org/abs/2212.09058), a discrete label prediction based audio pre-training framework, ranks **1st** in the [AudioSet](https://paperswithcode.com/sota/audio-classification-on-audioset) and [ESC-50](https://paperswithcode.com/sota/audio-classification-on-esc-50) audio classification leaderboards. We released the [codes and pre-trained models](https://aka.ms/beats).
   
   - [Sep 2022] [**SpeechLM**](https://arxiv.org/abs/2209.15329), a textual enhanced speech pre-training model, achieves **16%** relative WER reduction over data2vec with only **10K** text sentences on the LibriSpeech speech recognition benchmark.  We released the [codes and pre-trained models](https://aka.ms/speechlm).
   
   - [Sep 2022] [**WavLM**](https://ieeexplore.ieee.org/document/9814838) is published in IEEE Journal of Selected Topics in Signal Processing.
   
   - [Jan 2022] [**WavLM**](https://arxiv.org/abs/2110.13900) ranks **1st** in the [VoxSRC 2021 speaker verification permanent leaderboard](https://competitions.codalab.org/competitions/34066#results).
      
   - [Dec 2021] [**WavLM**](https://arxiv.org/abs/2110.13900) demo of speaker verification is on [Huggingface](https://huggingface.co/spaces/microsoft/wavlm-speaker-verification).
      
   - [Nov 2021] [**WavLM**](https://arxiv.org/abs/2110.13900) codes and pre-trained models are released [here](https://aka.ms/wavlm).
      
   - [Oct 2021] [**WavLM**](https://arxiv.org/abs/2110.13900) ranks **1st** in the [SUPERB leaderboard](https://superbbenchmark.org/leaderboard).
   
   - [Oct 2021] [**WavLM**](https://arxiv.org/abs/2110.13900), a large-scale self-supervised pre-training framework for full-stack speech processing, achieves **state-of-the-art** performance on **19** tasks, including all the 15 tasks on SUPERB benchmark, VoxCeleb1 speaker verification benchmark, LibriCSS speech separation benchmark, CALLHOME speech diarization benchmark and LibriSpeech speech recognition benchmark.
   
   - [Oct 2021] [**Ultra fast continuous speech separation model**](https://www.isca-speech.org/archive/pdfs/interspeech_2021/chen21l_interspeech.pdf) is **shipped** in the Microsoft Conversation Transcription Service.
   
   - [Dec 2020] [**Our continuous speech separation model**](https://ieeexplore.ieee.org/document/9413423) is **shipped** in the Microsoft Conversation Transcription Service.
      
   - [Oct 2020] [**Microsoft speaker diarization system**](https://arxiv.org/abs/2010.11458) with conformer-based continuous speech separation ranks **1st** in the [VoxCeleb Speaker Recognition Challenge 2020](https://competitions.codalab.org/competitions/26357#results).
      
   - [Aug 2020] [**Continuous speech separation with conformer**](https://ieeexplore.ieee.org/document/9413423) achieves **state-of-the-art** performance on the LibriCSS speech separation benchmark. We released the [codes and pre-trained models](https://github.com/Sanyuan-Chen/CSS_with_Conformer). See demos [here](https://www.youtube.com/watch?v=WRfPBnWc2qQ&t=3s).
   
   - [Apr 2020] [**RecAdam**](https://aclanthology.org/2020.emnlp-main.634), my 1st first-author paper, achieves **state-of-the-art** performance on the GLUE benchmark.  We released the [codes](https://github.com/Sanyuan-Chen/RecAdam). 

<!--
**Sanyuan-Chen/Sanyuan-Chen** is a âœ¨ _special_ âœ¨ repository because its `README.md` (this file) appears on your GitHub profile.

Here are some ideas to get you started:

- ðŸ”­ Iâ€™m currently working on ...
- ðŸŒ± Iâ€™m currently learning ...
- ðŸ‘¯ Iâ€™m looking to collaborate on ...
- ðŸ¤” Iâ€™m looking for help with ...
- ðŸ’¬ Ask me about ...
- ðŸ“« How to reach me: ...
- ðŸ˜„ Pronouns: ...
- âš¡ Fun fact: ...
- ðŸ“­ More about me: 
-->
